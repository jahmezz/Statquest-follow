{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9992ccd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'seed_everything' from 'pytorch_lightning.utilities.seed' (C:\\Users\\James Kahng\\AppData\\Roaming\\Python\\Python310\\site-packages\\pytorch_lightning\\utilities\\seed.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mL\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TensorDataset, DataLoader\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m seed_everything\n\u001b[0;32m     11\u001b[0m logger \u001b[38;5;241m=\u001b[39m TensorBoardLogger(save_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlightning_logs/\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_experiment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'seed_everything' from 'pytorch_lightning.utilities.seed' (C:\\Users\\James Kahng\\AppData\\Roaming\\Python\\Python310\\site-packages\\pytorch_lightning\\utilities\\seed.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import lightning as L\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=\"lightning_logs/\", name=\"my_experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2564aa4a-5c45-459f-bd30-df1d31b5997e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e22d84e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMbyHand(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        mean = torch.tensor(0.0)\n",
    "        std = torch.tensor(1.0)\n",
    "        \n",
    "        self.wlr1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wlr2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.blr1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "        \n",
    "        self.wpr1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wpr2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bpr1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "        \n",
    "        self.wp1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wp2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bp1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "        \n",
    "        self.wo1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wo2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bo1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "        \n",
    "    def lstm_unit(self, input_value, long_memory, short_memory):\n",
    "        long_remember_percent = torch.sigmoid((short_memory * self.wlr1) +\n",
    "                                             (input_value * self.wlr2) +\n",
    "                                             self.blr1)\n",
    "        \n",
    "        potential_remember_percent = torch.sigmoid((short_memory * self.wpr1) +\n",
    "                                                  (input_value * self.wpr2) +\n",
    "                                                  self.bpr1)\n",
    "        \n",
    "        potential_memory = torch.tanh((short_memory *self.wp1) +\n",
    "                                     (input_value * self.wp2) +\n",
    "                                     self.bp1)\n",
    "        \n",
    "        updated_long_memory = ((long_memory * long_remember_percent) +\n",
    "                              (potential_remember_percent * potential_memory))\n",
    "        \n",
    "        output_percent = torch.sigmoid((short_memory * self.wo1) +\n",
    "                                      (input_value * self.wo2) +\n",
    "                                      self.bo1)\n",
    "        \n",
    "        updated_short_memory = torch.tanh(updated_long_memory) * output_percent\n",
    "        \n",
    "        return ([updated_long_memory, updated_short_memory])\n",
    "    \n",
    "    def forward(self, input):\n",
    "        long_memory = 0\n",
    "        short_memory = 0\n",
    "        day1, day2, day3, day4 = input\n",
    "        \n",
    "        long_memory, short_memory = self.lstm_unit(day1, long_memory, short_memory)\n",
    "        long_memory, short_memory = self.lstm_unit(day2, long_memory, short_memory)\n",
    "        long_memory, short_memory = self.lstm_unit(day3, long_memory, short_memory)\n",
    "        long_memory, short_memory = self.lstm_unit(day4, long_memory, short_memory)\n",
    "        \n",
    "        return short_memory\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters())\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_i, label_i = batch\n",
    "        output_i = self.forward(input_i[0])\n",
    "        loss = (output_i - label_i)**2\n",
    "        \n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "        if(label_i == 0):\n",
    "            self.log(\"out_0\", output_i)\n",
    "        else:\n",
    "            self.log(\"out_1\", output_i)\n",
    "            \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c23b3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now let's compare  the observed  and predicted values...\n",
      "Company A: Observed = 0, Predicted = tensor(-0.4599)\n",
      "Company B: Observed = 1, Predicted = tensor(-0.4617)\n"
     ]
    }
   ],
   "source": [
    "model = LSTMbyHand()\n",
    "\n",
    "print(\"\\nNow let's compare  the observed  and predicted values...\")\n",
    "\n",
    "print(\"Company A: Observed = 0, Predicted =\",\n",
    "     model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "\n",
    "print(\"Company B: Observed = 1, Predicted =\",\n",
    "     model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d0b520d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "------------------------------\n",
      "12        Trainable params\n",
      "0         Non-trainable params\n",
      "12        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "C:\\Users\\James Kahng\\AppData\\Roaming\\Python\\Python310\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\James Kahng\\AppData\\Roaming\\Python\\Python310\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe305c4d36b2407d96290c9c2352e3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2000` reached.\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[0., 0.5, 0.25, 1.], [1., 0.5, 0.25, 1.]])\n",
    "labels = torch.tensor([0., 1.])\n",
    "\n",
    "dataset = TensorDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset)\n",
    "\n",
    "trainer = L.Trainer(max_epochs=2000, logger=logger)\n",
    "trainer.fit(model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df24c213-84d6-48be-8d20-826ae021a939",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at lightning_logs/my_experiment\\version_0\\checkpoints\\epoch=1999-step=4000.ckpt\n",
      "C:\\Users\\James Kahng\\AppData\\Roaming\\Python\\Python310\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:337: UserWarning: The dirpath has changed from 'lightning_logs/my_experiment\\\\version_0\\\\checkpoints' to 'C:\\\\Users\\\\James Kahng\\\\Code\\\\Statquest-follow\\\\lightning_logs\\\\version_13\\\\checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "------------------------------\n",
      "12        Trainable params\n",
      "0         Non-trainable params\n",
      "12        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "Restored all states from the checkpoint at lightning_logs/my_experiment\\version_0\\checkpoints\\epoch=1999-step=4000.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new trainer will start where the last left off, and the check point data is here: lightning_logs/my_experiment\\version_0\\checkpoints\\epoch=1999-step=4000.ckpt\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b140decb5184b0eb62e23868810ec57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3000` reached.\n"
     ]
    }
   ],
   "source": [
    "path_to_checkpoint = trainer.checkpoint_callback.best_model_path ## By default, \"best\" = \"most recent\"\n",
    "print(\"The new trainer will start where the last left off, and the check point data is here: \" + \n",
    "      path_to_checkpoint + \"\\n\")\n",
    "\n",
    "## Then create a new Lightning Trainer\n",
    "trainer = L.Trainer(max_epochs=3000) # Before, max_epochs=2000, so, by setting it to 3000, we're adding 1000 more.\n",
    "## And then call fit() using the path to the most recent checkpoint files\n",
    "## so that we can pick up where we left off.\n",
    "trainer.fit(model, train_dataloaders=dataloader, ckpt_path=path_to_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "421ccc40-660e-4fd8-9803-909131dc12d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at C:\\Users\\James Kahng\\Code\\Statquest-follow\\lightning_logs\\version_14\\checkpoints\\epoch=4999-step=10000.ckpt\n",
      "C:\\Users\\James Kahng\\AppData\\Roaming\\Python\\Python310\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:337: UserWarning: The dirpath has changed from 'C:\\\\Users\\\\James Kahng\\\\Code\\\\Statquest-follow\\\\lightning_logs\\\\version_14\\\\checkpoints' to 'C:\\\\Users\\\\James Kahng\\\\Code\\\\Statquest-follow\\\\lightning_logs\\\\version_15\\\\checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "------------------------------\n",
      "12        Trainable params\n",
      "0         Non-trainable params\n",
      "12        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "Restored all states from the checkpoint at C:\\Users\\James Kahng\\Code\\Statquest-follow\\lightning_logs\\version_14\\checkpoints\\epoch=4999-step=10000.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new trainer will start where the last left off, and the check point data is here: C:\\Users\\James Kahng\\Code\\Statquest-follow\\lightning_logs\\version_14\\checkpoints\\epoch=4999-step=10000.ckpt\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09209a15a66c487d82ceb6414f58a90d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10000` reached.\n"
     ]
    }
   ],
   "source": [
    "## First, find where the most recent checkpoint files are stored\n",
    "path_to_checkpoint = trainer.checkpoint_callback.best_model_path ## By default, \"best\" = \"most recent\"\n",
    "print(\"The new trainer will start where the last left off, and the check point data is here: \" + \n",
    "      path_to_checkpoint + \"\\n\")\n",
    "\n",
    "## Then create a new Lightning Trainer\n",
    "trainer = L.Trainer(max_epochs=10000) # Before, max_epochs=3000, so, by setting it to 5000, we're adding 2000 more.\n",
    "## And then call fit() using the path to the most recent checkpoint files\n",
    "## so that we can pick up where we left off.\n",
    "trainer.fit(model, train_dataloaders=dataloader, ckpt_path=path_to_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33bc1717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now let's compare  the observed  and predicted values...\n",
      "Company A: Observed = 0, Predicted = tensor(0.5045)\n",
      "Company B: Observed = 1, Predicted = tensor(0.5640)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNow let's compare  the observed  and predicted values...\")\n",
    "\n",
    "print(\"Company A: Observed = 0, Predicted =\",\n",
    "     model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "\n",
    "print(\"Company B: Observed = 1, Predicted =\",\n",
    "     model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c56ca0a-faee-406e-871d-6c9556c2030c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After optimization, the parameters are...\n",
      "wlr1 tensor(4.4318)\n",
      "wlr2 tensor(-0.2603)\n",
      "blr1 tensor(1.2237)\n",
      "wpr1 tensor(3.8003)\n",
      "wpr2 tensor(2.6243)\n",
      "bpr1 tensor(-0.7490)\n",
      "wp1 tensor(2.7579)\n",
      "wp2 tensor(0.6778)\n",
      "bp1 tensor(-0.2562)\n",
      "wo1 tensor(4.0243)\n",
      "wo2 tensor(1.5770)\n",
      "bo1 tensor(0.4162)\n"
     ]
    }
   ],
   "source": [
    "print(\"After optimization, the parameters are...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d96bddf-4f6f-45ed-b8ad-2e81a45cb25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instead of coding an LSTM by hand, let's see what we can do with PyTorch's nn.LSTM()\n",
    "class LightningLSTM(L.LightningModule):\n",
    "\n",
    "    def __init__(self): # __init__() is the class constructor function, and we use it to initialize the Weights and Biases.\n",
    "        \n",
    "        super().__init__() # initialize an instance of the parent class, LightningModule.\n",
    "\n",
    "        L.seed_everything(42)\n",
    "        \n",
    "        ## input_size = number of features (or variables) in the data. In our example\n",
    "        ##              we only have a single feature (value)\n",
    "        ## hidden_size = this determines the dimension of the output\n",
    "        ##               in other words, if we set hidden_size=1, then we have 1 output node\n",
    "        ##               if we set hiddeen_size=50, then we hve 50 output nodes (that can then be 50 input\n",
    "        ##               nodes to a subsequent fully connected neural network.\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=1) \n",
    "         \n",
    "\n",
    "    def forward(self, input):\n",
    "        ## transpose the input vector\n",
    "        input_trans = input.view(len(input), 1)\n",
    "        \n",
    "        lstm_out, temp = self.lstm(input_trans)\n",
    "        \n",
    "        ## lstm_out has the short-term memories for all inputs. We make our prediction with the last one\n",
    "        prediction = lstm_out[-1] \n",
    "        return prediction\n",
    "        \n",
    "        \n",
    "    def configure_optimizers(self): # this configures the optimizer we want to use for backpropagation.\n",
    "        return Adam(self.parameters(), lr=0.1) ## we'll just go ahead and set the learning rate to 0.1\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx): # take a step during gradient descent.\n",
    "        input_i, label_i = batch # collect input\n",
    "        output_i = self.forward(input_i[0]) # run input through the neural network\n",
    "        loss = (output_i - label_i)**2 ## loss = squared residual\n",
    "        \n",
    "        ###################\n",
    "        ##\n",
    "        ## Logging the loss and the predicted values so we can evaluate the training\n",
    "        ##\n",
    "        ###################\n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "        if (label_i == 0):\n",
    "            self.log(\"out_0\", output_i)\n",
    "        else:\n",
    "            self.log(\"out_1\", output_i)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a64e1b0e-ba39-45ba-8c77-6f95c47f4bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before optimization, the parameters are...\n",
      "lstm.weight_ih_l0 tensor([[ 0.7645],\n",
      "        [ 0.8300],\n",
      "        [-0.2343],\n",
      "        [ 0.9186]])\n",
      "lstm.weight_hh_l0 tensor([[-0.2191],\n",
      "        [ 0.2018],\n",
      "        [-0.4869],\n",
      "        [ 0.5873]])\n",
      "lstm.bias_ih_l0 tensor([ 0.8815, -0.7336,  0.8692,  0.1872])\n",
      "lstm.bias_hh_l0 tensor([ 0.7388,  0.1354,  0.4822, -0.1412])\n",
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company A: Observed = 0, Predicted = tensor([0.6675])\n",
      "Company B: Observed = 1, Predicted = tensor([0.6665])\n"
     ]
    }
   ],
   "source": [
    "model = LightningLSTM() # First, make model from the class\n",
    "\n",
    "## print out the name and value for each parameter\n",
    "print(\"Before optimization, the parameters are...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)\n",
    "    \n",
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(\"Company A: Observed = 0, Predicted =\", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(\"Company B: Observed = 1, Predicted =\", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4547763-6f50-479a-a76e-2d3c89766f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "0 | lstm | LSTM | 16    \n",
      "------------------------------\n",
      "16        Trainable params\n",
      "0         Non-trainable params\n",
      "16        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1148f371b0749f982836075279df4ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After optimization, the parameters are...\n",
      "lstm.weight_ih_l0 tensor([[3.5364],\n",
      "        [1.3869],\n",
      "        [1.5390],\n",
      "        [1.2488]])\n",
      "lstm.weight_hh_l0 tensor([[5.2070],\n",
      "        [2.9577],\n",
      "        [3.2652],\n",
      "        [2.0678]])\n",
      "lstm.bias_ih_l0 tensor([-0.9143,  0.3724, -0.1815,  0.6376])\n",
      "lstm.bias_hh_l0 tensor([-1.0570,  1.2414, -0.5685,  0.3092])\n"
     ]
    }
   ],
   "source": [
    "## NOTE: Because we have set Adam's learning rate to 0.1, we will train much, much faster.\n",
    "## Before, with the hand made LSTM and the default learning rate, 0.001, it took about 5000 epochs to fully train\n",
    "## the model. Now, with the learning rate set to 0.1, we only need 300 epochs. Now, because we are doing so few epochs,\n",
    "## we have to tell the trainer add stuff to the log files every 2 steps (or epoch, since we have to rows of training data)\n",
    "## because the default, updating the log files every 50 steps, will result in a terrible looking graphs. So\n",
    "trainer = L.Trainer(max_epochs=300, log_every_n_steps=2)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=dataloader)\n",
    "\n",
    "print(\"After optimization, the parameters are...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "631d01c8-f695-4abd-8e9c-88309017a67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company A: Observed = 0, Predicted = tensor([6.7751e-05])\n",
      "Company B: Observed = 1, Predicted = tensor([0.9809])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(\"Company A: Observed = 0, Predicted =\", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(\"Company B: Observed = 1, Predicted =\", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
